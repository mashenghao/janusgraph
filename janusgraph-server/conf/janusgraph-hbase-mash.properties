storage.backend=hbase
storage.hostname=dn4,dn3,dn5
storage.port=2181
#cache.db-cache=true
cache.db-cache-clean-wait=20
cache.db-cache-time=180000
#cache.db-cache-size=0.25
#storage.hbase.table=oracle:experiment_211
#storage.hbase.table=oracle:experiment_211
storage.hbase.table=oracle:experiment_402
ids.block-size=5000000
#storage.buffer-size=2560
#storage.batch-loading=true
#schema.default=none
#schema.constraints=false



#gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
#gremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.hbase.HBaseInputFormat
#gremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat
#gremlin.hadoop.jarsInDistributedCache=true
#gremlin.hadoop.inputLocation=persist_rdd
#gremlin.hadoop.outputLocation=persist_rdd
#gremlin.spark.persistContext=true
#gremlin.spark.graphStorageLevel=DISK_ONLY


# These properties defines the connection properties which were used while write data to JanusGraph.
#janusgraphmr.ioformat.conf.storage.backend=hbase
# This specifies the hostname & port for Cassandra data store.

#hbase的机器ip
#janusgraphmr.ioformat.conf.storage.hostname=dn3,dn4,dn5
#zookeeper的端口
#janusgraphmr.ioformat.conf.storage.port=2181

#hbase的表名(也是平台的图的id)
#janusgraphmr.ioformat.conf.storage.hbase.table=default:experiment_442

# SparkGraphComputer Configuration
#
#spark.master=yarn

#合理配置spark任务的资源
#spark.executor.memory=8g
#spark.executor.instances=4
#spark.executor.cores=2
#spark.deploy-mode=cluster
#spark.driver.maxResultSize=4g
#spark.rdd.compress=true
#spark.network.timeout=10000000
#spark.rpc.askTimeout=1000
#spark.serializer=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoSerializer

#按实际路径配置该jar包,janusgraph下的otherlib中
#spark.yarn.jars=/home/hadoop/janusgraph-0.4.0-hadoop2/otherlib/janusgraph-spark-1.0.2.jar
#spark.kryo.registrationRequired=false
