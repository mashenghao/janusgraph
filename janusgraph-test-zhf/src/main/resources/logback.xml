<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="30 seconds" debug="false" >
	<!-- 输出到控制台 -->
	<appender name="STDOUT"
		class="ch.qos.logback.core.ConsoleAppender">
		<encoder>
			<!-- 输出格式 -->
			<pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %-4relative [%15.15thread] %30.30logger{35} : %msg%n</pattern>
		</encoder>
	</appender>
	
	<!-- 异步输出 ，默认输出debug级别日志-->  
     <appender name ="ASYNC" class= "ch.qos.logback.classic.AsyncAppender">  
            <!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 -->  
            <discardingThreshold >0</discardingThreshold>  
            <!-- 更改默认的队列的深度,该值会影响性能.默认值为256 -->  
            <queueSize>512</queueSize>  
            <!-- 添加附加的appender,最多只能添加一个 -->  
         <appender-ref ref ="debugAppender"/>  
     </appender>  
	
	
	<!-- debug 日志 ,可以打开下面注释开启相关日志，无需重启，30秒后生效-->
	<!-- 图规则模块mybatis日志 -->
    <!-- <logger name="cn.com.bsfit.frms.graph.rule.mapper" level="DEBUG" /> -->
    <!-- ds入库日志 -->
    <!-- <logger name="cn.com.bsfit.frms.graph.ds.mapper" level="DEBUG" /> -->
    <!-- 图数据库入库debug log -->
    <!-- <logger name="cn.com.bsfit.frms.association.analysis.service.impl.DsImportServiceImpl" level="DEBUG" /> -->
    <logger name="org.apache.hadoop.yarn.util.RackResolver" level="WARN" />
	<logger name="org.apache.spark.scheduler.TaskSetManager" level="WARN" />
	<logger name="org.apache.spark.storage.BlockManagerInfo" level="WARN" />
	<logger name="org.apache.spark.ContextCleaner" level="WARN" />
	<logger name="org.apache.spark.scheduler.DAGScheduler" level="WARN" />
    <logger name="org.apache.spark.executor.Executor" level="WARN" />
    <logger name="org.apache.spark.storage.memory.MemoryStore" level="WARN" />
    <logger name="org.apache.spark.storage.ShuffleBlockFetcherIterator" level="WARN" />
    <logger name="org.apache.spark.storage.BlockManager" level="WARN" />
    <logger name="org.apache.spark.scheduler.TaskSchedulerImpl" level="WARN" />
    <logger name="org.spark_project.jetty.server.handler.ContextHandler" level="WARN" />
	<logger name="org.apache.hadoop.hbase.shaded.org.apache.zookeeper.ZooKeeper" level="WARN" />
	<logger name="org.apache.hadoop.hbase.shaded.org.apache.zookeeper.ClientCnxn" level="WARN" />
	<logger name="org.apache.tinkerpop.gremlin.process.computer.traversal.WorkerExecutor" level="INFO" />
	<logger name="org.apache.tinkerpop.gremlin.process.computer.traversal.MasterExecutor" level="INFO" />
	<logger name="org.apache.tinkerpop.gremlin.spark.process.computer.SparkExecutor" level="DEBUG" />
	<logger name="org.apache.tinkerpop.gremlin.spark.process.computer.SparkGraphComputer" level="DEBUG" />
	<logger name="org.apache.tinkerpop.gremlin.process.computer.clustering.peerpressure.PeerPressureVertexProgram" level="DEBUG" />
	<logger name="org.apache.tinkerpop.gremlin.process.traversal.step.map.GroupStep" level="INFO" />
	<logger name="org.apache.tinkerpop.gremlin.process.traversal.step.map.SelectStep" level="INFO" />
	<logger name="org.apache.tinkerpop.gremlin.process.computer.traversal.TraversalVertexProgram" level="INFO" />
	<!-- 根，所有logger的祖先 -->
	<root level="WARN">
		<appender-ref ref="STDOUT" />
		<appender-ref ref="errorAppender" />
		<appender-ref ref="infoAppender" />
		<appender-ref ref="warnAppender" />
	</root>
</configuration>
