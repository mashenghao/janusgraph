# read-cassandra-3.properties
#
# Hadoop Graph Configuration
#
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.hbase.HBaseInputFormat
gremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat
#gremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.graphson.GraphSONOutputFormat
gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.inputLocation=none
gremlin.hadoop.outputLocation=output
gremlin.spark.persistContext=true
gremlin.spark.graphStorageLevel=DISK_ONLY
#
# JanusGraph Cassandra InputFormat configuration
#
# These properties defines the connection properties which were used while write data to JanusGraph.
janusgraphmr.ioformat.conf.storage.backend=hbase
# This specifies the hostname & port for Cassandra data store.
janusgraphmr.ioformat.conf.storage.hostname=dn4,dn5,dn3
janusgraphmr.ioformat.conf.storage.port=2181
# This specifies the keyspace where data is stored.
janusgraphmr.ioformat.conf.storage.hbase.table=tmptest1
#janusgraphmr.ioformat.conf.storage.hbase.table=tmptest6
# This defines the indexing backned configuration used while writing data to JanusGraph.
#janusgraphmr.ioformat.conf.index.search.backend=elasticsearch
#janusgraphmr.ioformat.conf.index.search.hostname=10.100.1.16,10.100.1.21,10.100.1.22,10.100.1.23,10.100.1.24
# Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr).

#
# Apache Cassandra InputFormat configuration
#
#cassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner

#
# SparkGraphComputer Configuration
#
spark.master=local[1]
spark.executor.memory=1g
spark.driver.memory=10g
spark.driver.cores=5
spark.driver.maxResultSize=8g
spark.yarn.am.cores=2
spark.yarn.am.memory=4g
spark.executor.instances=5
spark.executor.cores=1
spark.deploy-mode=cluster
#spark.yarn.queue=q11
#spark.default.parallelism=200
#spark.driver.memory=1g
spark.network.timeout=10000000
spark.rpc.askTimeout=1000
spark.rdd.compress=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
#spark.serializer=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoSerializer
spark.yarn.jars=D:\\git\\janusgraph-spark\\target\\janusgraph-spark-1.0.2.jar
spark.yarn.dist.files=D:\\git\\janusgraph\\janusgraph-test-zhf\\src\\main\\resources\\logback.xml
#spark.yarn.am.extraJavaOptions=-Djava.library.path=/home/hadoop/hadoop/lib/native
#spark.yarn.appMasterEnv.JAVA_HOME=/home/hadoop/jdk1.8.0_151/jre
#spark.executorEnv.JAVA_HOME=/home/hadoop/jdk1.8.0_151/jre
#spark.executor.extraClassPath=/home/hadoop/janusgraph-0.4.0-hadoop2/otherlib/janusgraph-spark-1.0.2.jar
#spark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator
#spark.kryo.registrationRequired=false
#spark.executor.extraJavaOptions=-Dlog4j.configuration=/home/hadoop/janusgraph-0.4.0-hadoop2/conf/gremlin-server/log4j-server.properties